{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Getting Started with Echoflow on AWS: Processing EK60 Data to compute MVBS\n",
    "\n",
    "\n",
    "Welcome to the Echoflow AWS Deployment Guide notebook! This notebook will walk you through the steps to deploy an Echoflow pipeline on Amazon Web Services (AWS). Echoflow is a powerful tool for acoustics data processing and analysis, and AWS provides a scalable and reliable cloud platform for running your workflows.\n",
    "\n",
    "In this notebook, you will learn how to set up an EC2 instance, install Echoflow, configure AWS S3 storage for your processed data, and initiate your Echoflow pipeline using Prefect. By following these steps, you'll be able to seamlessly run your acoustics data processing workflows in the cloud.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, make sure you have the following prerequisites in place:\n",
    "\n",
    "- An AWS account with necessary permissions to create EC2 instances and S3 buckets.\n",
    "- A private key (.pem) file for SSH access to your EC2 instance.\n",
    "- Basic familiarity with the command-line interface (CLI).\n",
    "\n",
    "## Notebook Setup\n",
    "\n",
    "In this notebook, we'll go through the following steps:\n",
    "\n",
    "1. Create an EC2 Instance: Set up an AWS EC2 instance to run your Echoflow pipeline.\n",
    "2. Connect to EC2 using SSH: Establish a secure connection to your EC2 instance using SSH.\n",
    "3. Install Echoflow: Create a virtual environment, clone the Echoflow repository, and install the package.\n",
    "4. Initialize Echoflow and Prefect: Configure your Echoflow and Prefect environments.\n",
    "5. Create an S3 Bucket: Set up an AWS S3 bucket to store your processed data.\n",
    "6. Store AWS Credentials: Store your AWS credentials securely for access.\n",
    "7. Create Credential Blocks: Set up credential blocks for secure access within Prefect.\n",
    "8. Open Jupyter Notebook: Launch Jupyter Notebook to execute your Echoflow pipeline.\n",
    "9. Setting Up: Setting up source S3.\n",
    "10. Getting Data: Get data from the source S3.\n",
    "11. Preparing Files: Preparing files for processing.\n",
    "12. Processing with echoflow: Execute the Pipeline.\n",
    "\n",
    "Now, let's dive into each step to deploy your Echoflow pipeline on AWS!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create an EC2 Instance\n",
    "Refer to the [AWS EC2] (https://docs.aws.amazon.com/efs/latest/ug/gs-step-one-create-ec2-resources.html) documentation for a step-by-step guide on creating an EC2 instance. Make sure you have the private key (.pem) file for SSH access.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 2: Connect to EC2 using SSH\n",
    "Connect to your EC2 instance using SSH. You can follow the instructions in the [AWS SSH] (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-linux-inst-ssh.html) documentation. You'll need the private key (.pem) file.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 3: Install Echoflow\n",
    "\n",
    "### Step 3.1: Make a virtual environment\n",
    "\n",
    "To keep your Echoflow environment isolated, it's recommended to create a virtual environment using Conda or Python's built-in venv module. Here's an example using Conda:\n",
    "\n",
    "```bash\n",
    "conda create --name echoflow-env\n",
    "conda activate echoflow-env\n",
    "```\n",
    "\n",
    "Or, using Python's venv:\n",
    "\n",
    "```bash\n",
    "python -m venv echoflow-env\n",
    "source echoflow-env/bin/activate  # On Windows, use `echoflow-env\\Scripts\\activate`\n",
    "```\n",
    "### Step 3.2: Clone the Repository\n",
    "Now that you have a virtual environment set up, you can clone the Echoflow project repository to your local machine using the following command:\n",
    "\n",
    "```bash\n",
    "git clone <echoflow_repo>\n",
    "```\n",
    "\n",
    "### Step 3.3: Install the Package\n",
    "Navigate to the project directory you've just cloned and install the Echoflow package. The -e flag is crucial as it enables editable mode, which is especially helpful during development and testing. Now, take a moment and let the echoflow do its thing while you enjoy your coffee.\n",
    "\n",
    "```bash\n",
    "cd <project_directory>\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Initialize Echoflow and Prefect\n",
    "\n",
    "To kickstart your journey with Echoflow and Prefect, follow these simple initialization steps:\n",
    "\n",
    "##### 4.1 Initializing Echoflow\n",
    "Begin by initializing Echoflow with the following command:\n",
    "\n",
    "```bash\n",
    "echoflow init\n",
    "```\n",
    "\n",
    "This command sets up the groundwork for your Echoflow environment, preparing it for seamless usage.\n",
    "\n",
    "##### 4.2 Initializing Prefect\n",
    "For Prefect, initialization involves a few extra steps, including secure authentication. Enter the following command to initiate the Prefect authentication process:\n",
    "\n",
    "- If you have a Prefect Cloud account, provide your Prefect API key to securely link your account. Type your API key when prompted and press Enter.\n",
    "\n",
    "```bash\n",
    "prefect cloud login\n",
    "```\n",
    "\n",
    "- If you don't have a Prefect Cloud account yet, you can use local prefect account. This is especially useful for those who are just starting out and want to explore Prefect without an account.\n",
    "\n",
    "```bash\n",
    "prefect profiles create echoflow-local\n",
    "```\n",
    "\n",
    "\n",
    "The initialization process will ensure that both Echoflow and Prefect are properly set up and ready for you to dive into your cloud-based workflows.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 5: Create a S3 bucket to store the output\n",
    "\n",
    "Create an S3 bucket to store the processed output. Refer to the [AWS S3] (https://docs.aws.amazon.com/quickstarts/latest/s3backup/step-1-create-bucket.html) documentation for guidance. Add the S3 URI to datastore.yaml in the same directory as this notebook under the urlpath key in the output section:\n",
    "\n",
    "```yaml\n",
    "# ...rest of the cofiguration\n",
    "output: # Output arguments\n",
    "  urlpath: <YOUR_S3_URI> # Destination data URL parameters\n",
    "  overwrite: true \n",
    "  storage_options: \n",
    "    block_name: echoflow-aws-credentials\n",
    "    type: AWS\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Step 6: Store AWS Credentials\n",
    "\n",
    "Edit the ~/.echoflow/credentials.ini file and add your AWS Key and Secret.\n",
    "\n",
    "```bash\n",
    "nano ~/.echoflow/credentials.ini\n",
    "\n",
    "# add the following and save:\n",
    "[echoflow-aws-credentials]\n",
    "aws_access_key_id=my-aws-key\n",
    "aws_secret_access_key=my-aws-secret\n",
    "provider=AWS\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Step 7: Create Credential blocks\n",
    "\n",
    "Once you have stored the credentials in the ini file, call the below command to create a block securedly stored in your prefect account. For more about blocks refer [Blocks] (https://github.com/OSOceanAcoustics/echoflow/blob/dev/docs/configuration/blocks.md). \n",
    "\n",
    "```bash\n",
    "echoflow load-credentials\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Step 8: Jupyter Notebook\n",
    "Open Jupyter Notebook using terminal in the same activated environment \n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Setting Up\n",
    "We begin by importing the required libraries and specifying the paths for the dataset and pipeline configuration files. These files contain the necessary information for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from echoflow import echoflow_start, StorageType, glob_url\n",
    "\n",
    "dataset_config = Path(\"./datastore.yaml\").resolve()\n",
    "pipeline_config = Path(\"./pipeline.yaml\").resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9.1\n",
    "### Pipeline Configuration: Mean Volume Backscattering Strength\n",
    "In this section, we will provide you with the pipeline configuration that we'll be using for our MVBS processing. The configuration is presented in YAML format, which is a structured and human-readable way to define settings for data processing.\n",
    "\n",
    "Here's the configuration we'll be using:\n",
    "\n",
    "```yaml\n",
    "active_recipe: standard \n",
    "use_local_dask: true\n",
    "n_workers: 3\n",
    "pipeline:\n",
    "- recipe_name: standard \n",
    "  stages: \n",
    "  - name: echoflow_open_raw \n",
    "    module: echoflow.stages.subflows.open_raw \n",
    "    options: \n",
    "      save_raw_file: true\n",
    "      use_raw_offline: true \n",
    "      use_offline: true \n",
    "  - name: echoflow_combine_echodata\n",
    "    module: echoflow.stages.subflows.combine_echodata\n",
    "    options:\n",
    "      use_offline: true\n",
    "  - name: echoflow_compute_SV\n",
    "    module: echoflow.stages.subflows.compute_SV\n",
    "    options:\n",
    "      use_offline: true\n",
    "  - name: echoflow_compute_MVBS\n",
    "    module: echoflow.stages.subflows.compute_MVBS\n",
    "    options:\n",
    "      use_offline: true\n",
    "    external_params:\n",
    "      range_meter_bin: 20 \n",
    "      ping_time_bin: 20S\n",
    "\n",
    "```\n",
    "    \n",
    "Let's break down the components of this configuration:\n",
    "\n",
    "- **active_recipe**: Specifies the recipe to be used for processing, which is set as \"standard\" in this case.\n",
    "\n",
    "- **use_local_dask**: This flag indicates that we'll be utilizing a local Dask Cluster for parallel processing.\n",
    "\n",
    "- **n_workers**: Determines the number of worker processes in the Dask Cluster. Here, we're using 3 workers for efficient parallelization.\n",
    "\n",
    "- **pipeline**: This section defines the sequence of stages to execute. In this example, we're following the \"standard\" recipe, which comprises four stages.\n",
    "\n",
    "    - **echoflow_open_raw**: This stage utilizes the `open_raw` subflow module to open raw data files. It includes options such as saving raw files, using raw data in offline mode, and utilizing offline data.\n",
    "    \n",
    "    - **echoflow_combine_echodata**: This stage employs the `combine_echodata` subflow module to combine echodatas based on transect. It includes an option to use offline data.\n",
    "    \n",
    "    - **compute_SV**: This stage employs the `compute_SV` subflow module to compute Backscattering Strength. It includes an option to use offline data.\n",
    "    \n",
    "    - **compute_MVBS**: This stage employs the `compute_MVBS` subflow module to calculate MVBS. It includes an option to use offline data.\n",
    "\n",
    "**Note**: For a more comprehensive understanding of each option and its functionality, you can refer to the [Pipeline documentation](https://github.com/OSOceanAcoustics/echoflow/blob/dev/docs/configuration/pipeline.md).\n",
    "\n",
    "Keep in mind that in this example, we'll be setting up a local Dask Cluster with 3 workers for parallel processing. This configuration will enable us to efficiently process our data for MVBS analysis. To turn it off, toggle `use_local_dask` to false.\n",
    "\n",
    "Feel free to explore and modify the configuration to understand better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9.2\n",
    "### Datastore Configuration: Organizing Data for Processing\n",
    "\n",
    "In this section, we'll delve into the configuration that defines how the data will be organized and managed for processing. This configuration is provided in YAML format and plays a crucial role in structuring data inputs and outputs.\n",
    "\n",
    "Here's the detailed breakdown of the configuration:\n",
    "\n",
    "```yaml\n",
    "name: Bell_M._Shimada-SH1707-EK60\n",
    "sonar_model: EK60 \n",
    "raw_regex: (.*)-?D(?P<date>\\w{1,8})-T(?P<time>\\w{1,6}) \n",
    "args: \n",
    "  urlpath: s3://ncei-wcsd-archive/data/raw/{{ ship_name }}/{{ survey_name }}/{{ sonar_model }}/*.raw\n",
    "  parameters:\n",
    "    ship_name: Bell_M._Shimada\n",
    "    survey_name: SH1707\n",
    "    sonar_model: EK60\n",
    "  storage_options:\n",
    "    anon: true\n",
    "  transect:\n",
    "    file: ./EK60_SH1707_Shimada.txt\n",
    "    default_transect_num: 2017\n",
    "  json_export: true \n",
    "output: \n",
    "  urlpath: <YOUR-S3-BUCKET>\n",
    "  retention: false\n",
    "  overwrite: true\n",
    "  storage_options: \n",
    "    block_name: echoflow-aws-credentials\n",
    "    type: AWS\n",
    "```\n",
    "\n",
    "Let's delve into the individual components of the configuration presented here:\n",
    "\n",
    "- **name**: Specifies a descriptive name for the configuration, aiding in identifying its purpose.\n",
    "\n",
    "- **sonar_model**: Indicates the type of sonar being utilized, which in this case is \"EK60\".\n",
    "\n",
    "- **raw_regex**: Defines a regular expression pattern for extracting date and time information from raw data file names.\n",
    "\n",
    "- **args**: This section provides crucial arguments for structuring data inputs:\n",
    "\n",
    "  - **urlpath**: Defines the URL pattern to access the raw data files stored on a remote server. The placeholders `{{ ship_name }}`, `{{ survey_name }}`, and `{{ sonar_model }}` are dynamically replaced with the specified values.\n",
    "\n",
    "  - **storage_options**: Sets storage options, such as anonymous access (`anon: true`), for retrieving the data.\n",
    "\n",
    "  - **transect**: Specifies a file (`EK60_SH1707_Shimada.txt`) containing the list of files to process, along with default transect information.\n",
    "\n",
    "  - **json_export**: Enables JSON metadata export.\n",
    "\n",
    "- **output**: This section configures the output settings for processed data:\n",
    "\n",
    "  - **urlpath**: Determines the output directory (`<YOUR-S3-BUCKET>`) where the processed data will be stored.\n",
    "\n",
    "  - **retention**: Disables data retention, indicating that only MVBS data will be stored in this case.\n",
    "\n",
    "  - **overwrite**: Allows data overwriting if the data already exists.\n",
    "\n",
    "**Note**: \n",
    "- For a more comprehensive understanding of each option and its functionality, you can refer to the [Datast documentation](https://github.com/OSOceanAcoustics/echoflow/blob/dev/docs/configuration/datastore.md).\n",
    "- The pipeline will store MVBS Strength output under `<YOUR-S3-BUCKET>`. As the retention is set to false, only MVBS Strength files will be stored. To specify files for processing, create a list of file names and store it in `EK60_SH1707_Shimada.txt`, which should be placed under the transect directory.\n",
    "\n",
    "This configuration facilitates efficient data organization and management for the processing pipeline. Feel free to tailor it to your specific data and processing requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Getting Data\n",
    "Next, we'll use the glob_url function to retrieve a list of URLs matching a specific pattern. In this case, we're targeting raw EK60 data files from the SH1707 survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob_url(\"s3://ncei-wcsd-archive/data/raw/Bell_M._Shimada/SH1707/EK60/*.raw\", {'anon':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Preparing Files\n",
    "We'll now extract the file names from the URLs and create a file listing for the transect. This will help us organize and work with the data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for file in all_files:\n",
    "    f = file.split(\".r\")[0]\n",
    "    files.append(f.split(\"/\")[-1])\n",
    "\n",
    "transect = open('EK60_SH1707_Shimada.txt','w')\n",
    "i = 0\n",
    "for f in files:\n",
    "    if i == 5:\n",
    "        break\n",
    "    transect.write(f+\".raw\\n\")\n",
    "    i = i + 1\n",
    "transect.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Processing with echoflow\n",
    "Now, we're ready to kick off the data processing using echoflow. We'll provide the dataset and pipeline configurations, along with additional options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 16:01:16,445 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7jjfqrq8', purging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:35413' processes=3 threads=6, memory=15.61 GiB>\n",
      "--------------------------------------------------\n",
      "\n",
      "Executing stage :  name='echoflow_open_raw' module='echoflow.stages.subflows.open_raw' external_params=None options={'save_raw_file': True, 'use_raw_offline': True, 'use_offline': True} prefect_config=None\n",
      "[Errno 111] Connection refused\n",
      "\n",
      "Completed stage name='echoflow_open_raw' module='echoflow.stages.subflows.open_raw' external_params=None options={'save_raw_file': True, 'use_raw_offline': True, 'use_offline': True} prefect_config=None\n",
      "--------------------------------------------------\n",
      "<Client: 'tcp://127.0.0.1:35413' processes=3 threads=6, memory=15.61 GiB>\n",
      "--------------------------------------------------\n",
      "\n",
      "Executing stage :  name='echoflow_combine_echodata' module='echoflow.stages.subflows.combine_echodata' external_params=None options={'use_offline': True} prefect_config=None\n",
      "[Errno 111] Connection refused\n",
      "Cleaning :  s3://echoflow-workground/combined_files_dask/echoflow_open_raw\n",
      "s3://echoflow-workground/combined_files_dask/echoflow_open_raw\n",
      "Failed to cleanup s3://echoflow-workground/combined_files_dask/echoflow_open_raw\n",
      "\n",
      "Completed stage name='echoflow_combine_echodata' module='echoflow.stages.subflows.combine_echodata' external_params=None options={'use_offline': True} prefect_config=None\n",
      "--------------------------------------------------\n",
      "<Client: 'tcp://127.0.0.1:35413' processes=3 threads=6, memory=15.61 GiB>\n",
      "--------------------------------------------------\n",
      "\n",
      "Executing stage :  name='echoflow_compute_SV' module='echoflow.stages.subflows.compute_SV' external_params=None options={'use_offline': True} prefect_config=None\n",
      "[Errno 111] Connection refused\n",
      "Cleaning :  s3://echoflow-workground/combined_files_dask/echoflow_combine_echodata\n",
      "s3://echoflow-workground/combined_files_dask/echoflow_combine_echodata\n",
      "Failed to cleanup s3://echoflow-workground/combined_files_dask/echoflow_combine_echodata\n",
      "\n",
      "Completed stage name='echoflow_compute_SV' module='echoflow.stages.subflows.compute_SV' external_params=None options={'use_offline': True} prefect_config=None\n",
      "--------------------------------------------------\n",
      "<Client: 'tcp://127.0.0.1:35413' processes=3 threads=6, memory=15.61 GiB>\n",
      "--------------------------------------------------\n",
      "\n",
      "Executing stage :  name='echoflow_compute_MVBS' module='echoflow.stages.subflows.compute_MVBS' external_params={'range_meter_bin': 20, 'ping_time_bin': '20S'} options={'use_offline': True} prefect_config=None\n",
      "[Errno 111] Connection refused\n",
      "Cleaning :  s3://echoflow-workground/combined_files_dask/echoflow_compute_SV\n",
      "Cleanup complete\n",
      "\n",
      "Completed stage name='echoflow_compute_MVBS' module='echoflow.stages.subflows.compute_MVBS' external_params={'range_meter_bin': 20, 'ping_time_bin': '20S'} options={'use_offline': True} prefect_config=None\n",
      "--------------------------------------------------\n",
      "Local Client has been closed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7087/3922359326.py:2: RuntimeWarning: coroutine 'Block.load' was never awaited\n",
      "  data  = echoflow_start(dataset_config=dataset_config, pipeline_config=pipeline_config, options=options)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "options = {\"storage_options_override\": False}\n",
    "data  = echoflow_start(dataset_config=dataset_config, pipeline_config=pipeline_config, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Results\n",
    "Finally, let's take a look at the first entry from the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<xarray.Dataset>\n",
       "  Dimensions:            (channel: 3, ping_time: 456, echo_range: 38)\n",
       "  Coordinates:\n",
       "    * channel            (channel) <U37 'GPT  18 kHz 009072058c8d 1-1 ES18-11' ...\n",
       "    * echo_range         (echo_range) float64 0.0 20.0 40.0 ... 700.0 720.0 740.0\n",
       "    * ping_time          (ping_time) datetime64[ns] 2017-06-15T19:02:00 ... 201...\n",
       "  Data variables:\n",
       "      Sv                 (channel, ping_time, echo_range) float64 dask.array<chunksize=(2, 456, 38), meta=np.ndarray>\n",
       "      frequency_nominal  (channel) float64 dask.array<chunksize=(3,), meta=np.ndarray>\n",
       "  Attributes:\n",
       "      processing_function:          commongrid.compute_MVBS\n",
       "      processing_software_name:     echopype\n",
       "      processing_software_version:  0.7.2.dev51+gb45c942\n",
       "      processing_time:              2023-08-30T15:54:41Z]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
