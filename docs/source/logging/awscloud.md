# Implementing the Logging System

The logging system in echodataflow is implemented using Pythonâ€™s built-in logging module. The logger is configured to send logs to various destinations based on the configuration provided. Adding CloudWatch logging for your echodataflow application is straightforward.

## AWS CloudWatch
### Setting Up AWS CloudWatch
First, configure AWS CloudWatch by following the instructions provided in the (AWS CloudWatch Getting Setup Guide)[https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingSetup.html].

**Note**: You also need to set up the AWS CLI and ensure your Python environment has the `watchtower` package installed.

You can find the steps for setting up the AWS CLI (here)[https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html].

To install the watchtower package, run:

```bash
pip install watchtower
```

### Example

#### Configuring AWS CloudWatch Logging
Create a logging YAML file to configure AWS CloudWatch logging for echodataflow. Below is an example configuration:

```yaml
version: 1
disable_existing_loggers: False
formatters:
  json:
    format: "[%(asctime)s] %(process)d %(levelname)s %(name)s:%(funcName)s:%(lineno)s - %(message)s"
  plaintext:
    format: "[%(asctime)s] %(process)d %(levelname)s %(name)s:%(funcName)s:%(lineno)s - %(message)s"
handlers:
  echodataflow_watchtower:
    class: watchtower.CloudWatchLogHandler
    formatter: json
    level: DEBUG
    log_group_name: echodataflow_logs
    log_stream_name: echodataflow_stream
    send_interval: 10
    create_log_group: False
  logfile:
    class: logging.handlers.RotatingFileHandler
    formatter: plaintext
    level: DEBUG
    filename: echodataflow.log
    maxBytes: 1000000
    backupCount: 3
loggers:
  echodataflow:
    level: DEBUG
    propagate: False
    handlers: [echodataflow_watchtower, logfile]
```

#### Integrating the Logging Configuration
Finally, pass this YAML configuration file along with the dataset and pipeline configuration when initializing echodataflow.

Here is an example of how to integrate the logging configuration into your echodataflow initialization:

```python
from pathlib import Path
import yaml
import logging.config

# Load logging configuration
logging_config_path = Path("./logging.yaml").resolve()
with open(logging_config_path, 'r') as f:
    logging_config = yaml.safe_load(f)

# Set up logging
logging.config.dictConfig(logging_config)
logger = logging.getLogger('echodataflow')

# Example initialization of echodataflow
dataset_config = 'path_to_dataset_config'
pipeline_config = 'path_to_pipeline_config'
options = {}  # Add your options here

data = echodataflow_start(dataset_config=dataset_config, pipeline_config=pipeline_config, logging_config=logging_config, options=options)

logger.info("Echodataflow started successfully.")
```

## Dask Streams for Logging
echodataflow is configured to use Dask streams for logging by default. This ensures that logs generated by Dask workers are captured and can be redirected to a local file or any other logging destination configured in the logging YAML. By default if logging is not configured, all the worker messages are directed to application console.

### Dask Streams Configuration
In the above configuration, logs from Dask streams will be written to a local file (configured with a rotating file handler). However, unlike CloudWatch, the order of logs may not be preserved since logs are written once control returns from the Dask workers to the main application.

## Summary
1. Console: Logs are written to the application console by default.
2. AWS CloudWatch Logging: Logs are sent to AWS CloudWatch for centralized logging and monitoring.
3. Dask Streams: Dask streams are configured by default in echodataflow. Logs from Dask workers are captured and written to the configured handlers. Note that the order of logs might not be strictly maintained when written to local files.